import numpy as np #for data processing - linear algebra
import pandas as pd #for data processing - csv file
import seaborn as sns #for visualization
import matplotlib.pyplot as plt #for visualization
from matplotlib.table import table

from sklearn.model_selection import train_test_split #for machine learning models
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report,
                             precision_score, recall_score, f1_score ,roc_auc_score)
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.multiclass import OneVsRestClassifier  # Ø¨Ø±Ø§ÛŒ SVM Ú†Ù†Ø¯Ú©Ù„Ø§Ø³Ù‡
from sklearn.preprocessing import label_binarize

import os #for file managment
from imblearn.over_sampling import SMOTE

#ØªÙ†Ø¸ÛŒÙ… Ù†Ù…Ø§ÛŒØ´ Ú©Ø§Ù…Ù„ Ø¬Ø¯ÙˆÙ„â€ŒÙ‡Ø§
pd.set_option('display.max_columns', None)  # Ù†Ø´ÙˆÙ† Ø¯Ø§Ø¯Ù† Ù‡Ù…Ù‡ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
pd.set_option('display.width', 1000)        # Ø¹Ø±Ø¶ Ø¬Ø¯ÙˆÙ„ Ø¯Ø± Ø®Ø±ÙˆØ¬ÛŒ
pd.set_option('display.max_colwidth', None) # Ù†Ù…Ø§ÛŒØ´ Ú©Ø§Ù…Ù„ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¯Ø§Ø®Ù„ Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§ (Ø¨Ø±Ø§ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ†ÛŒ)
pd.set_option('display.max_rows', None)

#A1---------------------------------------
print("â‰ï¸ Ø¨Ø®Ø´ Ø§Ù„Ù - Û±:")
dataset = pd.read_csv('milling_machine.csv')
print(dataset.head(10))
dataset.info()
print(dataset.describe())

#A2----------------------------------------
print("â‰ï¸ Ø¨Ø®Ø´ Ø§Ù„Ù - Û²:")
missing_count = dataset.isnull().sum()
missing_ratio = (missing_count / len(dataset)) *100
missing_df = pd.DataFrame({
    'Missing Count': missing_count,
    'Missing Ratio (%)': missing_ratio
})
print(missing_df)

#A3---------------------------------------------
print("â‰ï¸ Ø¨Ø®Ø´ Ø§Ù„Ù - Û³:")
# ØªØ¨Ø¯ÛŒÙ„ Failure Types Ø¨Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¹Ø¯Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ
dataset['Failure Types (Encoded)'] = dataset['Failure Types'].astype('category').cat.codes
correlation_matrix = dataset.corr(numeric_only=True)

print(correlation_matrix)

plt.figure(figsize=(10,8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

#A4------------------------------------------------------------
print("â‰ï¸ Ø¨Ø®Ø´ Ø§Ù„Ù - Û´:")
important_features = ['Tool Wear (Seconds)', 'Air Temp (Â°C)', 'Torque (Nm)']

for feature in important_features:
    plt.figure(figsize=(10,6))
    sns.histplot(dataset[feature], bins=30, kde=True, color='skyblue')
    plt.title(f'Distribution of {feature}')
    plt.xlabel(feature)
    plt.ylabel('Count')
    plt.grid(True)
    plt.show()

#B1---------------------------------------------------------
print("â‰ï¸ Ø¨Ø®Ø´ Ø¨ - Û±:")
df = dataset.copy()

# Ù„ÛŒØ³Øª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ (Ø¨Ø¯ÙˆÙ† Ø³ØªÙˆÙ† Failure Types Ùˆ Ø¨Ø¯ÙˆÙ† Ù†Ø³Ø®Ù‡ Encode Ø´Ø¯Ù‡)
features = ['Air Temp (Â°C)', 'Process Temp (Â°C)', 'Rotational Speed (RPM)', 'Torque (Nm)', 'Tool Wear (Seconds)']
classes = df['Failure Types'].unique()

# Ø³Ø§Ø®Øª Ø¬Ø¯ÙˆÙ„ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ù„Ø§Ø³
mean_class = pd.DataFrame(index=features, columns=classes)

# Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙˆÛŒÚ˜Ú¯ÛŒ Ø¯Ø± Ù‡Ø± Ú©Ù„Ø§Ø³
for f in classes:
    class_data = df[df['Failure Types'] == f]
    mean_values = class_data[features].mean()
    mean_class.loc[:, f] = mean_values

# Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…ÙˆÙ‚Ø¹ÛŒØª Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡
nan_locations = []
for index, row in df.iterrows():
    for col in features:
        if pd.isnull(row[col]):
            nan_locations.append((index, col))

# Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…Ø´Ø¯Ù‡ Ø¨Ø§ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù‡Ù…Ø§Ù† Ú©Ù„Ø§Ø³
for (index, column) in nan_locations:
    class_label = df.loc[index, 'Failure Types']
    df.loc[index, column] = mean_class.loc[column, class_label]

# Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ú©Ø±Ø¯Ù† Ø¯ÛŒØªØ§Ø³Øª Ø§ØµÙ„ÛŒ Ø¨Ø§ Ù†Ø³Ø®Ù‡ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒâ€ŒØ´Ø¯Ù‡
dataset = df.copy()
print(dataset.isnull().sum())
# Ø­Ø°Ù Ø±Ø¯ÛŒÙâ€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Failure Types Ù†Ø§Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ù†
dataset = dataset.dropna(subset=['Failure Types']).reset_index(drop=True)
print("\n number of missing values in failure type: " + str(dataset['Failure Types'].isnull().sum()))

#B2--------------------------------------------------------------
print("â‰ï¸ Ø¨Ø®Ø´ Ø¨ - Û²:")
features_to_scale = ['Air Temp (Â°C)', 'Process Temp (Â°C)', 'Rotational Speed (RPM)', 'Torque (Nm)', 'Tool Wear (Seconds)']
dataset_scaled = dataset.copy()

scaler = StandardScaler()
dataset_scaled[features_to_scale] = scaler.fit_transform(dataset_scaled[features_to_scale])

print(dataset_scaled[features_to_scale].describe())

# Ø³Ø§Ø®Øª Ø³ØªÙˆÙ† Ø§Ù†Ú©Ø¯ Ø´Ø¯Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ø§Ø² Failure Types Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ
dataset_scaled['Failure Types (Encoded)'] = dataset_scaled['Failure Types'].astype('category').cat.codes

dataset_scaled.info()

#j1---------------------------------------------------------------
print("â‰ï¸ Ø¨Ø®Ø´ Ø¬ - Û±:")
# Ø³Ø§Ø®Øª Ø³ØªÙˆÙ† Ø¯ÙˆØ¯Ø³ØªÙ‡â€ŒØ§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ Ù†Ø§Ù… 'Binary Failure'
dataset_scaled['Binary Failure'] = dataset_scaled['Failure Types'].apply(
    lambda x: 0 if x == 'No Failure' else 1
)
print(dataset_scaled[['Failure Types', 'Binary Failure']].head(10))
print(dataset_scaled['Binary Failure'].value_counts())

#j2---------------------------------------------------------------
print("â‰ï¸ Ø¨Ø®Ø´ Ø¬ - Û²:")
sns.countplot(x='Binary Failure', hue='Binary Failure', data=dataset_scaled, palette='deep', legend=False)
plt.title('Distribution of Binary Failure Classes')
plt.xlabel('Failure Class (0 = No Failure, 1 = Failure)')
plt.ylabel('Count')
plt.grid(True, axis='y')
plt.show()

#j3-----------------------------------------------------------------
print("â‰ï¸ Ø¨Ø®Ø´ Ø¬ - Û³:")
print(dataset_scaled['Binary Failure'].value_counts(normalize=True))

#j4-----------------------------------------------------------------
print("â‰ï¸ Ø¨Ø®Ø´ Ø¬ - Û´:")
X = dataset_scaled[features_to_scale]
y = dataset_scaled['Binary Failure']

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

X_resampled_df = pd.DataFrame(X_resampled, columns=features_to_scale)
y_resampled_df = pd.DataFrame(y_resampled, columns=['Binary Failure'])

print(y_resampled_df['Binary Failure'].value_counts())

#j5---------------------------------------------------------
print("â‰ï¸ Ø¨Ø®Ø´ Ø¬ - Ûµ:")
X_train, X_test, y_train, y_test = train_test_split(
    X_resampled_df,  # ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
    y_resampled_df,  # Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§
    test_size=0.2,
    random_state=42,
    stratify=y_resampled_df  # Ø¨Ø±Ø§ÛŒ Ø­ÙØ¸ Ù†Ø³Ø¨Øª Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§
)

print("Train shape:", X_train.shape, y_train.shape)
print("Test shape:", X_test.shape, y_test.shape)

# Logistic Regression
logistic_model = LogisticRegression(random_state=42)
logistic_model.fit(X_train, y_train.values.ravel())

# K-Nearest Neighbors
knn_model = KNeighborsClassifier()
knn_model.fit(X_train, y_train.values.ravel())

# SVM - Linear kernel
svm_linear = SVC(kernel='linear', random_state=42)
svm_linear.fit(X_train, y_train.values.ravel())

# SVM - RBF kernel (ØºÛŒØ±Ø®Ø·ÛŒ)
svm_rbf = SVC(kernel='rbf', random_state=42)
svm_rbf.fit(X_train, y_train.values.ravel())

#j6-------------------------------------------------------------------
print("â‰ï¸ Ø¨Ø®Ø´ Ø¬ - Û¶:")
models = {
    "Logistic Regression": logistic_model,
    "KNN": knn_model,
    "SVM (Linear)": svm_linear,
    "SVM (RBF)": svm_rbf
}
results = []

for name, model in models.items():
    y_pred = model.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)
    report = classification_report(y_test, y_pred, output_dict=True)

    print(f"\nModel: {name}")
    print("Confusion Matrix:\n", cm)
    print("Classification Report:\n", classification_report(y_test, y_pred))

    results.append({
        "Model": name,
        "Accuracy": round(acc, 4),
        "Precision (class 1)": round(report['1']['precision'], 4),
        "Recall (class 1)": round(report['1']['recall'], 4),
        "F1-score (class 1)": round(report['1']['f1-score'], 4)
    })

results_df = pd.DataFrame(results)
print("\n\nðŸ“Š Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§:\n")
print(results_df)

#j7---------------------------------------------------------------------
print("â‰ï¸ Ø¨Ø®Ø´ Ø¬ - Û·:")
knn = KNeighborsClassifier()

param_grid_knn = {
    'n_neighbors': [3, 5, 7, 9, 11]
}

grid_search_knn = GridSearchCV(knn, param_grid_knn, scoring='accuracy', cv=5)
grid_search_knn.fit(X_train, y_train.values.ravel())

print("KNN - Best K:", grid_search_knn.best_params_)
print("Best Accuracy:", round(grid_search_knn.best_score_, 4))

best_knn = grid_search_knn.best_estimator_

log_reg = LogisticRegression(solver='liblinear', random_state=42)

param_grid_log = {
    'C': [0.01, 0.1, 1, 10],
    'penalty': ['l1', 'l2']
}

grid_search_log = GridSearchCV(log_reg, param_grid_log, scoring='accuracy', cv=5)
grid_search_log.fit(X_train, y_train.values.ravel())

print("Logistic Regression - Best Params:", grid_search_log.best_params_)
print("Best Accuracy:", round(grid_search_log.best_score_, 4))

best_log_model = grid_search_log.best_estimator_

svm_linear = SVC(kernel='linear', random_state=42)

param_grid_linear = {
    'C': [0.01, 0.1, 1, 10]
}

grid_search_linear = GridSearchCV(svm_linear, param_grid_linear, scoring='accuracy', cv=5)
grid_search_linear.fit(X_train, y_train.values.ravel())

print("SVM (Linear) - Best Params:", grid_search_linear.best_params_)
print("Best Accuracy:", round(grid_search_linear.best_score_, 4))

best_svm_linear = grid_search_linear.best_estimator_

svm_rbf = SVC(kernel='rbf', random_state=42)

param_grid_rbf = {
    'C': [0.1, 1, 10],
    'gamma': [0.01, 0.1, 1]
}

grid_search_rbf = GridSearchCV(svm_rbf, param_grid_rbf, scoring='accuracy', cv=5)
grid_search_rbf.fit(X_train, y_train.values.ravel())

print("SVM (RBF) - Best Params:", grid_search_rbf.best_params_)
print("Best Accuracy:", round(grid_search_rbf.best_score_, 4))

best_svm_rbf = grid_search_rbf.best_estimator_

#j8------------------------------------------------------------------------
print("â‰ï¸ Ø¨Ø®Ø´ Ø¬ - Û¸:")
optimized_models = {
    "KNN (Optimized)": best_knn,
    "Logistic Regression (Optimized)": best_log_model,
    "SVM (Linear, Optimized)": best_svm_linear,
    "SVM (RBF, Optimized)": best_svm_rbf
}
metrics_results = []

for name, model in optimized_models.items():
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None

    acc = accuracy_score(y_test, y_pred)
    err = 1 - acc
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    # Confusion matrix: [TN, FP, FN, TP]
    cm = confusion_matrix(y_test, y_pred)
    tn, fp, fn, tp = cm.ravel()
    specificity = tn / (tn + fp)

    # AUC only if model supports predict_proba
    auc = roc_auc_score(y_test, y_prob) if y_prob is not None else "N/A"

    metrics_results.append({
        "Model": name,
        "Accuracy": round(acc, 4),
        "Error Rate": round(err, 4),
        "Precision (class 1)": round(precision, 4),
        "Recall (class 1)": round(recall, 4),
        "Specificity (class 0)": round(specificity, 4),
        "F1-score (class 1)": round(f1, 4),
        "AUC-ROC": round(auc, 4) if auc != "N/A" else "N/A",
        "Confusion Matrix": f"{cm.tolist()}"
    })

metrics_df = pd.DataFrame(metrics_results)
pd.set_option("display.max_colwidth", None)
print("ðŸ“Š Ø¬Ø¯ÙˆÙ„ Ù†Ù‡Ø§ÛŒÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø§ ØªÙ…Ø§Ù… Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§:\n")
print(metrics_df)

#d1---------------------------------------------------------------------------
print("â‰ï¸ Ø¨Ø®Ø´ Ø¯ - Û±:")
# X Ùˆ y Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ú†Ù†Ø¯Ú©Ù„Ø§Ø³Ù‡
X_multi = dataset_scaled[features_to_scale]
y_multi = dataset_scaled['Failure Types (Encoded)']

X_train_mc, X_test_mc, y_train_mc, y_test_mc = train_test_split(
    X_multi, y_multi, test_size=0.2, random_state=42, stratify=y_multi)

# 1. KNN
knn_mc = KNeighborsClassifier(n_neighbors=5)
knn_mc.fit(X_train_mc, y_train_mc)

# 2. Decision Tree
dt_mc = DecisionTreeClassifier(random_state=42)
dt_mc.fit(X_train_mc, y_train_mc)

# 3. Random Forest
rf_mc = RandomForestClassifier(n_estimators=100, random_state=42)
rf_mc.fit(X_train_mc, y_train_mc)

# 4. SVM Ø¨Ø§ One-vs-Rest (Ø¨Ø±Ø§ÛŒ Ú†Ù†Ø¯Ú©Ù„Ø§Ø³Ù‡)
svm_mc = OneVsRestClassifier(SVC(kernel='rbf', probability=True, random_state=42))
svm_mc.fit(X_train_mc, y_train_mc)

#d2-----------------------------------------------------------------------
print("â‰ï¸ Ø¨Ø®Ø´ Ø¯ - Û²:")

multi_models = {
    "KNN": knn_mc,
    "Decision Tree": dt_mc,
    "Random Forest": rf_mc,
    "SVM (OvR)": svm_mc
}
multi_results = []

for name, model in multi_models.items():
    y_pred = model.predict(X_test_mc)

    acc = accuracy_score(y_test_mc, y_pred)
    cm = confusion_matrix(y_test_mc, y_pred)
    report = classification_report(y_test_mc, y_pred, output_dict=True, zero_division=0)

    print(f"\nðŸ”¹ {name}")
    print("Confusion Matrix:")
    print(cm)
    print("Classification Report:")
    print(classification_report(y_test_mc, y_pred, zero_division=0))

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† macro Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù„ÛŒ
    multi_results.append({
        "Model": name,
        "Accuracy": round(acc, 4),
        "Macro F1": round(report['macro avg']['f1-score'], 4),
        "Macro Precision": round(report['macro avg']['precision'], 4),
        "Macro Recall": round(report['macro avg']['recall'], 4),
        "Confusion Matrix": cm.tolist()
    })

results_df_mc = pd.DataFrame(multi_results)
pd.set_option("display.max_colwidth", None)
print("\nðŸ“Š Ø¬Ø¯ÙˆÙ„ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ú†Ù†Ø¯Ú©Ù„Ø§Ø³Ù‡:")
print(results_df_mc)

#d3--------------------------------------------------------------------------
print("â‰ï¸ Ø¨Ø®Ø´ Ø¯ - Û³:")

knn = KNeighborsClassifier()

param_grid_knn = {
    'n_neighbors': [3, 5, 7, 9, 11]
}

grid_knn = GridSearchCV(knn, param_grid_knn, scoring='accuracy', cv=5)
grid_knn.fit(X_train_mc, y_train_mc)

print("KNN - Best Params:", grid_knn.best_params_)
print("Best Accuracy:", round(grid_knn.best_score_, 4))
best_knn_mc = grid_knn.best_estimator_


dt = DecisionTreeClassifier(random_state=42)

param_grid_dt = {
    'max_depth': [5, 10, 20, None],
    'criterion': ['gini', 'entropy']
}

grid_dt = GridSearchCV(dt, param_grid_dt, scoring='accuracy', cv=5)
grid_dt.fit(X_train_mc, y_train_mc)

print("Decision Tree - Best Params:", grid_dt.best_params_)
print("Best Accuracy:", round(grid_dt.best_score_, 4))
best_dt_mc = grid_dt.best_estimator_


rf = RandomForestClassifier(random_state=42)

param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 20, None]
}

grid_rf = GridSearchCV(rf, param_grid_rf, scoring='accuracy', cv=5)
grid_rf.fit(X_train_mc, y_train_mc)

print("Random Forest - Best Params:", grid_rf.best_params_)
print("Best Accuracy:", round(grid_rf.best_score_, 4))
best_rf_mc = grid_rf.best_estimator_


svm = OneVsRestClassifier(SVC(kernel='rbf', probability=True, random_state=42))

param_grid_svm = {
    'estimator__C': [0.1, 1, 10],
    'estimator__gamma': [0.01, 0.1, 1]
}

grid_svm = GridSearchCV(svm, param_grid_svm, scoring='accuracy', cv=5)
grid_svm.fit(X_train_mc, y_train_mc)

print("SVM (OvR) - Best Params:", grid_svm.best_params_)
print("Best Accuracy:", round(grid_svm.best_score_, 4))
best_svm_mc = grid_svm.best_estimator_

#d4----------------------------------------------------------------------------
print("â‰ï¸ Ø¨Ø®Ø´ Ø¯ - Û´:")
final_multi_models = {
    "KNN (opt)": best_knn_mc,
    "Decision Tree (opt)": best_dt_mc,
    "Random Forest (opt)": best_rf_mc,
    "SVM (OvR, opt)": best_svm_mc
}

# Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ Ø¨Ù‡ ØµÙˆØ±Øª one-hot Ø¨Ø±Ø§ÛŒ AUC
classes = np.unique(y_test_mc)
y_test_binarized = label_binarize(y_test_mc, classes=classes)

final_multi_results = []

for name, model in final_multi_models.items():
    y_pred = model.predict(X_test_mc)

    # Accuracy & Error
    acc = accuracy_score(y_test_mc, y_pred)
    err = 1 - acc

    # Precision, Recall, F1 (macro)
    precision = precision_score(y_test_mc, y_pred, average='macro', zero_division=0)
    recall = recall_score(y_test_mc, y_pred, average='macro', zero_division=0)
    f1 = f1_score(y_test_mc, y_pred, average='macro', zero_division=0)

    # Confusion Matrix
    cm = confusion_matrix(y_test_mc, y_pred)

    # Specificity (Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ú©Ù„ÛŒ Ø§Ø² Ù‡Ø± Ú©Ù„Ø§Ø³ØŒ Ù…Ø«Ù„ sensitivity Ø§Ù…Ø§ Ø¨Ø±Ø§ÛŒ Ú©Ù„Ø§Ø³ Ù…Ù†ÙÛŒ)
    # Ø¨Ø±Ø§ÛŒ Ú†Ù†Ø¯Ú©Ù„Ø§Ø³Ù‡ ØªØ¹Ø±ÛŒÙâ€ŒØ´Ø¯Ù‡ Ø¨Ù‡â€ŒØµÙˆØ±Øª TN / (TN + FP)
    specificity_list = []
    for i in range(len(classes)):
        TP = cm[i, i]
        FN = sum(cm[i, :]) - TP
        FP = sum(cm[:, i]) - TP
        TN = cm.sum() - (TP + FN + FP)
        specificity = TN / (TN + FP) if (TN + FP) > 0 else 0
        specificity_list.append(specificity)
    specificity_avg = np.mean(specificity_list)

    # AUC-ROC (Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯ Ø§Ø­ØªÙ…Ø§Ù„)
    if hasattr(model, "predict_proba"):
        y_score = model.predict_proba(X_test_mc)
        auc = roc_auc_score(y_test_binarized, y_score, multi_class='ovr', average='macro')
    else:
        auc = "N/A"

    final_multi_results.append({
        "Model": name,
        "Accuracy": round(acc, 4),
        "Error Rate": round(err, 4),
        "Precision (Macro)": round(precision, 4),
        "Recall (Macro)": round(recall, 4),
        "F1-score (Macro)": round(f1, 4),
        "Specificity (Macro)": round(specificity_avg, 4),
        "AUC-ROC": round(auc, 4) if auc != "N/A" else "N/A",
        "Confusion Matrix": cm.tolist()
    })

df_d4 = pd.DataFrame(final_multi_results)
pd.set_option('display.max_colwidth', None)
print("ðŸ“Š Ø¬Ø¯ÙˆÙ„ Ù†Ù‡Ø§ÛŒÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ú†Ù†Ø¯Ú©Ù„Ø§Ø³Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ´Ø¯Ù‡:")
print(df_d4)